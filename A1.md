BMEG 424 Assignment 1
================

- [BMEG 424 Assignment 0: Introduction and
  Pipelines](#bmeg-424-assignment-0-introduction-and-pipelines)
  - [Introduction:](#introduction)
    - [General Information About
      Assignments](#general-information-about-assignments)
    - [Data](#data)
    - [Software](#software)
    - [Goal](#goal)
    - [Submission:](#submission)
  - [Experiment and Analysis (20.75
    pts):](#experiment-and-analysis-2075-pts)
    - [Part 1: Pipelines](#part-1-pipelines)
      - [a. Setting up a pipeline](#a-setting-up-a-pipeline)
      - [b. Running the pipeline](#b-running-the-pipeline)
    - [Part 2: Quality Control](#part-2-quality-control)
      - [a. Quality Control of raw sequencing
        data](#a-quality-control-of-raw-sequencing-data)
    - [b. Quality control of the
      alignment](#b-quality-control-of-the-alignment)
    - [Part 3: Visualization and Downstream
      Analysis](#part-3-visualization-and-downstream-analysis)
      - [a. Setting up a reproducible analysis in
        R](#a-setting-up-a-reproducible-analysis-in-r)
      - [b. Basics of R Analysis](#b-basics-of-r-analysis)
      - [c. Interactively viewing our variants in
        IGV](#c-interactively-viewing-our-variants-in-igv)
- [Contributions](#contributions)
- [References:](#references)

# BMEG 424 Assignment 0: Introduction and Pipelines

## Introduction:

### General Information About Assignments

We would like to begin by reminding you of some of the key rules and
regulations you will have to follow for the assignments in this course:

- All assignments are to be completed individually or with one partner

- If you choose to work in pairs for an assignment, **ONE** of you
  should make a submission on canvas. In the submission you **MUST**
  report the name & student number of BOTH partners.

- Each time you work with a new partner you will need to create a new
  GitHub repo for that assignment. You should share your repos with the
  @BMEGGenoInfo account on Github so TAs have access to your submission.

- Late assignments will have 10% deducted for each day they are late and
  assignments will not be accepted \>3 days late. Making changes to the
  files on your repo after the assignment has been submitted on Canvas
  will result in a late penalty unless an instructor has specifically
  asked you to change something.

- Each student will have to ensure their assignment files are knit
  correctly (see below for instructions) and legible on GitHub. 20% or
  more of the assignment marks can be removed at the TA’s discretion for
  incorrect knitting or formatting of assignment files.

- We are aware that some of you will choose to use large language models
  like ChatGPT to complete some portions of the assignment. You are free
  to use ChatGPT or other LLMs to help you on the assignment, *provided
  you cite the model and a link to the conversation* (see
  [here](https://help.openai.com/en/articles/7925741-chatgpt-shared-links-faq)
  for how to do this with ChatGPT)

- More generally, it is a requirement that you include any and all
  sources from where you received help (excluding your previously
  mentioned partner) or information for completing any question on the
  assignments. If you are unsure of when/how to cite your sources
  (including LLMs), please get in contact with the instructors.

If any of these rules are unclear please get in contact with the
instructors.

### Data

The data for this assignment is located at
`/projects/bmeg/assignments/A1/`. You will be working with a subset of
sequencing data from a patient. The data is in the fastq format.

### Software

Before you begin working on this assignment, you’ll need to create a
conda environment with all of the relevant software tools installed. You
can do this by locating the yaml file in `/projects/bmeg/A1/` and
running
`conda env create -f /projects/bmeg/A1/A1_environment.yaml --prefix <path_to_your_env>`.
You can then activate your environment by running
`conda activate <path_to_your_env>`.

The tools you will be using for this assignment are: - fastQC
(<https://www.bioinformatics.babraham.ac.uk/projects/fastqc/>):
comprehensive quality control measures of sequencing data. - bowtie2
(<http://bowtie-bio.sourceforge.net/bowtie2/index.shtml>): alignments of
sequencing data. - htop & screen: bash system tools to visualize the
server capacity and to run commands in the background, respectively -
samtools (<http://www.htslib.org/>): manipulation of sequencing data. -
mpileup: a part of samtools which is used for variant calling from
sequencing data. Will be installed with samtools. - bcftools
(<http://www.htslib.org/>): a part of samtools which is used for variant
calling from sequencing data. Will be installed with samtools.

### Goal

The purpose of this assignment is to introduce you to the tools (command
line and R) and concepts (reproducibility, parallelization, data
manipulation etc.) that you will use throughout the course. This
assignment focuses on the construction of pipelines, which are useful
for ensuring your analysis is reproducible and efficient.

For this assignment we will be working with patient samples, and using
Snakemake to build a pipeline for calling variants from sequencing data.
Note that you really don’t need any understand of what variant calling
is to complete this assignment, and we will cover it in more detail
later in the course.

### Submission:

Submit your assignment as a knitted RMarkdown document. You will push
your knitted RMarkdown document to your github repository (one for each
group). You will then submit the link, along with the names and student
numbers of all students who worked on the assignment to the assignment 1
page on Canvas. Your assignment should be submitted, and your last
commit should be made, before 11:59pm on the day of the deadline. Late
assignments will will be deducted 10% per day late. Assignments will not
be accepted after 3 days past the deadline.

Please ensure you have knit your file to the “github_document” format
before submitting. If the file is not viewable on Github without
downloading, it will be considered improperly formated and will be
subject to the 20% markdown.

## Experiment and Analysis (20.75 pts):

### Part 1: Pipelines

Pipelines are a useful way to organize and automate your analysis of
data. They allow you to set up a series of steps in a predefined and
predictable order. This may seem like a waste of time on these
assignments where you are only analyzing a single or a handful of
samples, but it will be essential when you are analyzing hundreds or
thousands of samples.

#### a. Setting up a pipeline

For this course we will be using a pipeline manager called Snakemake
(<https://snakemake.readthedocs.io/en/stable/>). Snakemake is a python
based pipeline manager which will easily interface with conda and your
various software tools installed thereby. Snakemake also handles many of
the common issues in pipeline management like parallelization,
dependency management, error handling, and run
configuration/customization.

First we’ll need to add snakemake to our conda environment. You can do
this by typing `conda install -c bioconda snakemake` into the command
line.

First create a directory within your A1 directory called “pipeline”.
Then create a file called “Snakefile” within the pipeline directory.

Snakemake uses a python based syntax to define the steps in your
pipeline, these steps are called “rules”. Each rule has a set of inputs,
and a set of outputs. Rule are dependent on one another based on their
inputs and outputs. For example, if rule A has an output which is the
input for rule B, then rule B will not run until rule A has completed.
Rules which are not dependent on one another will run in parallel.

Create your Snakefile in the pipeline directory. Here is an example of
the rule for running fastqc on your input data (which should now be in
~/A1/).

    rule fastqc:
        input:
            "/projects/bmeg/A1/{sample}_1.fastq.gz",
            "/projects/bmeg/A1/{sample}_2.fastq.gz"
        output:
        # While in the A1 directory, so creating this directory in it.
            "fastqc/{sample}_1_fastqc.html",
            "fastqc/{sample}_2_fastqc.html"
        shell:
            "fastqc {input} --outdir fastqc"

You can see that the rule has an input and an output. The input is the
path to the raw sequencing data, and the output is the path to the
fastqc output. The shell command is the command that will be run when
the rule is executed.

#### b. Running the pipeline

Our pipeline will consist of the following steps: - Quality control of
the raw sequencing data (fastqc)

- Alignment of the sequencing data to the reference genome (bowtie2,
  paired-end using both forward and reverse reads and the hg38 genome)

- Sorting the aligned data (samtools)

- Variant calling (bcftools; installed with samtools)

It is up to you to implement the remaining steps:

    #?# 1. Fill in the rest of the Snakefile to include rules for sorting the aligned data, indexing the sorted data, and calling variants. (5 pts)

    configfile: "config.yaml"

    rule all:
        input:
            expand("fastqc/{sample}_1_fastqc.html", sample=config['samples']),
            expand("fastqc/{sample}_2_fastqc.html", sample=config['samples']),
            expand("aligned/{sample}.sam", sample=config['samples']),
            expand("sorted/{sample}.bam", sample=config['samples']),
            expand("sorted/{sample}.bam.bai", sample=config['samples']),
            expand("variants/{sample}.vcf", sample=config['samples'])

    rule fastqc:
        input:
            "/projects/bmeg/A1/{sample}_1.fastq.gz",
            "/projects/bmeg/A1/{sample}_2.fastq.gz"
        output:
            "fastqc/{sample}_1_fastqc.html",
            "fastqc/{sample}_2_fastqc.html"
        shell:
            "fastqc {input} --outdir fastqc"

    rule align:
        input:
            fastq1 = "/projects/bmeg/A1/{sample}_1.fastq.gz",
            fastq2 = "/projects/bmeg/A1/{sample}_2.fastq.gz"
        output:
            sam = "aligned/{sample}.sam"
        shell:
            "bowtie2 -x /projects/bmeg/indexes/hg38/hg38_bowtie2_index -1 {input.fastq1} -2 {input.fastq2} -S {output.sam}"

    rule samtools_sort:
        input:
            "aligned/{sample}.sam"
        output:
            bam = "sorted/{sample}.bam"
        shell:
            "samtools sort {input} -o {output}"

    rule samtools_index:
        input:
            bam = "sorted/{sample}.bam"
        output:
            bai = "sorted/{sample}.bam.bai"
        shell:
            "samtools index {input.bam}"

    rule bcftools_call:
        input:
            genome="/projects/bmeg/indexes/hg38/hg38.fa",
            bam="sorted/{sample}.bam",
            bai="sorted/{sample}.bam.bai"
        output:
            vcf="variants/{sample}.vcf"
        shell:
            "samtools mpileup -uf {input.genome} {input.bam} | "
            "bcftools call -mv -Ov -o {output.vcf}"

Before you start running your snakefile you will need to make sure it is
set up correctly. You can type `snakemake -np` into the command line to
check that your snakefile is set up correctly. Note this will only work
if your snakefile is called “Snakefile” and is in your current working
directory.

You should also check to see that the dependency map of your snakefile
is correct. Once you have confirmed that your snakefile is set up
correctly you can type `snakemake --dag | dot -Tsvg > dag.svg` into the
command line to generate a dependency map of your snakefile. You can
view the dependency map by typing `eog dag.svg` into the command line.

``` bash
#?# 2. Include the dependency map of your Snakefile below using Rmarkdown syntax (1 pts)
# The correct Rmarkdown syntax is ![NameOfImg](path/to/dag.svg) 
```

<figure>
<img src="/Users/mekail/Downloads/dag.svg" alt="dag" />
<figcaption aria-hidden="true">dag</figcaption>
</figure>

``` bash
#?# 3. Explain what the dependency map is showing and whether or not you think it is correct. (1 pts)
The dependency map shows the workflow of the Snakefile pipeline. It includes FastQC for quality control, alignment, sorting through Samtools sort,  indexing using Samtools index, and variant calling with Bcftools Call. Arrows represent dependencies, ensuring tasks occur in the correct sequence. 
```

Run your pipeline on the sequence data provided. You can do this by
typing `snakemake --use-conda --cores=1 --resources mem_mb=4000` into
the command line. You can use screen and htop to check the server usage
and determine the correct number of resources to allocate (NOTE: If you
don’t pass any resources to snakemake it will use all available
resources on the server, which is very inconsiderate to your
classmates).

**Please do not exceed 4GB of memory per job or a single core**, use
less if you think it’s necessary.

``` bash
#?# 4. Paste the output of snakemake here (0.25 pts)
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=4000
Job stats:
job               count
--------------  -------
align                 1
all                   1
bcftools_call         1
samtools_index        1
samtools_sort         1
total                 5

Select jobs to execute...

[Fri Jan 17 04:46:43 2025]
rule align:
    input: /projects/bmeg/A1/subset_SRR099957_1.fastq.gz, /projects/bmeg/A1/subset_SRR099957_2.fastq.gz
    output: aligned/subset_SRR099957.sam
    jobid: 2
    reason: Missing output files: aligned/subset_SRR099957.sam
    wildcards: sample=subset_SRR099957
    resources: tmpdir=/tmp

500000 reads; of these:
  500000 (100.00%) were paired; of these:
    50779 (10.16%) aligned concordantly 0 times
    333976 (66.80%) aligned concordantly exactly 1 time
    115245 (23.05%) aligned concordantly >1 times
    ----
    50779 pairs aligned concordantly 0 times; of these:
      5612 (11.05%) aligned discordantly 1 time
    ----
    45167 pairs aligned 0 times concordantly or discordantly; of these:
      90334 mates make up the pairs; of these:
        75415 (83.48%) aligned 0 times
        8614 (9.54%) aligned exactly 1 time
        6305 (6.98%) aligned >1 times
92.46% overall alignment rate
[Fri Jan 17 04:49:55 2025]
Finished job 2.
1 of 5 steps (20%) done
Select jobs to execute...

[Fri Jan 17 04:49:55 2025]
rule samtools_sort:
    input: aligned/subset_SRR099957.sam
    output: sorted/subset_SRR099957.bam
    jobid: 3
    reason: Missing output files: sorted/subset_SRR099957.bam; Input files updated by another job: aligned/subset_SRR099957.sam
    wildcards: sample=subset_SRR099957
    resources: tmpdir=/tmp

[Fri Jan 17 04:50:10 2025]
Finished job 3.
2 of 5 steps (40%) done
Select jobs to execute...

[Fri Jan 17 04:50:10 2025]
rule samtools_index:
    input: sorted/subset_SRR099957.bam
    output: sorted/subset_SRR099957.bam.bai
    jobid: 4
    reason: Missing output files: sorted/subset_SRR099957.bam.bai; Input files updated by another job: sorted/subset_SRR099957.bam
    wildcards: sample=subset_SRR099957
    resources: tmpdir=/tmp

[Fri Jan 17 04:50:11 2025]
Finished job 4.
3 of 5 steps (60%) done
Select jobs to execute...

[Fri Jan 17 04:50:11 2025]
rule bcftools_call:
    input: /projects/bmeg/indexes/hg38/hg38.fa, sorted/subset_SRR099957.bam, sorted/subset_SRR099957.bam.bai
    output: variants/subset_SRR099957.vcf
    jobid: 5
    reason: Missing output files: variants/subset_SRR099957.vcf; Input files updated by another job: sorted/subset_SRR099957.bam.bai, sorted/subset_SRR099957.bam
    wildcards: sample=subset_SRR099957
    resources: tmpdir=/tmp

[mpileup] 1 samples in 1 input files
Note: none of --samples-file, --ploidy or --ploidy-file given, assuming all sites are diploid
<mpileup> Set max per-file depth to 8000
[Fri Jan 17 04:54:35 2025]
Finished job 5.
4 of 5 steps (80%) done
Select jobs to execute...

[Fri Jan 17 04:54:35 2025]
localrule all:
    input: fastqc/subset_SRR099957_1_fastqc.html, fastqc/subset_SRR099957_2_fastqc.html, aligned/subset_SRR099957.sam, sorted/subset_SRR099957.bam, sorted/subset_SRR099957.bam.bai, variants/subset_SRR099957.vcf
    jobid: 0
    reason: Input files updated by another job: sorted/subset_SRR099957.bam.bai, aligned/subset_SRR099957.sam, sorted/subset_SRR099957.bam, variants/subset_SRR099957.vcf
    resources: tmpdir=/tmp

[Fri Jan 17 04:54:35 2025]
Finished job 0.
5 of 5 steps (100%) done
Complete log: .snakemake/log/2025-01-17T044641.827192.snakemake.log
```

### Part 2: Quality Control

#### a. Quality Control of raw sequencing data

Download the fastqc reports from the fastqc directory and include each
of the graphs in your Rmarkdown file the same way you included the
dependency map.

    #?# 5. Include the fastqc graphs from the QC on the forward read (read 1) file in your Rmarkdown file below this block. For each graph include a brief description of what the graph is showing and whether or not you think the data passed the quality control. (5 pts) 
    # Please try to separate your descriptions by including an text block between for description. 

<figure>
<img src="/Users/mekail/Downloads/BaseStats.png" alt="BaseStats" />
<figcaption aria-hidden="true">BaseStats</figcaption>
</figure>

![PerBaseSeq](/Users/mekail/Downloads/PerBaseSeq.png) \# Per Base
sequence Quality

    Description: The per base sequencing quality graph shows the average quality scores for bases across all reads represented as box blot distributions. 

    Assessment: The data fails quality control as the quality drops significantly at the end of the reads, entering the red region (low quality). Trimming may be required. The distribution is very large even from earlier positions in the read. 

![PerTileSeq](/Users/mekail/Downloads/PerTileSeq.png) \# Per Tile
Sequence Quality

    Description: It is a heatmap that represents sequencing quality across the flow cell tiles (physical sections of the sequencing chip). Each tile represents a vertical section of the graph. The colors are on a cold and hot scale, where the colder colors indicate better quality and warmer indicates worder tile quality

    Assessment: There are consistently blue tiles across the graph indicating there are no quality losses reported in any tile of the flowcell.

![PerSeq](/Users/mekail/Downloads/PerSeq.png) \# Per Sequence Quality
Scores

    Description: This graph is a plot of the total number of reads against the average quality score in Phred score over the full length of the read.

    Assessment: The data passes quality control since most sequences have high average quality scores, shown by the peak at the higher end of the graph. However, there is a spike in low quality reads, which means it would be good to trim the lower quality reads, or it would be good to investigate issues further. 

![PerBaseSeqContent](/Users/mekail/Downloads/PerBaseSeqContent.png) \#
Per Base Sequence Content

    Description: This plot shows the percentage of bases that were called for each nucleotide at each position in all reads.

    Assessment: The data passes quality control as the lines are relatively flat and balanced across positions. No significant bias is observed.

![PerSeqGCcontent](/Users/mekail/Downloads/PerSeqGCcontent.png) \# Per
Sequence GC Content

    Description: This is a plot of the number reads against the GC% per read. A theoretical normal distribution is also displayed for reference that assumes a uniform GC content for all reads.

    Assessment: The data has a slight warning as the observed GC content deviates slightly from the theoretical distribution. This may need further investigation but is not necessarily critical.

![PerBaseN](/Users/mekail/Downloads/PerBaseN.png) \# Per Base N Content

    Description: The percentage of bases at each position with no base call, labeled as N. 

    Assessment: The data passes quality control since the percentage of N content is negligible across all positions.

<figure>
<img src="/Users/mekail/Downloads/SeqLengthDist.png"
alt="SeqLengthDist" />
<figcaption aria-hidden="true">SeqLengthDist</figcaption>
</figure>

    #
    Description: This graph shows the distribution of reads lengths over all sequences.

    Assessment: The data passes quality control as the read lengths are consistent and expected (centered around 76 bp), as mentioned in the base stats, indicating it was intentionally set there.

<figure>
<img src="/Users/mekail/Downloads/SeqDupLevels.png"
alt="SeqDupLevels" />
<figcaption aria-hidden="true">SeqDupLevels</figcaption>
</figure>

    #
    Description: This plot shows the percentage of reads of a given sequence which are present more than once. High duplication levels can indicate PCR artifacts or low complexity.

    Assessment: The data passes quality control since the duplication levels are minimal, with most sequences being unique.

![OverrepresentedSeq](/Users/mekail/Downloads/OverrepresentedSeq.png) \#

    Description: This just lists sequences that appear more than expected.

    Assessment: The data has a warning due to the presence of an overrepresented sequence likely from the TruSeq adapter. It would be a good idea to perform adapter trimming.

<figure>
<img src="/Users/mekail/Downloads/AdapterContent.png"
alt="AdapterContent" />
<figcaption aria-hidden="true">AdapterContent</figcaption>
</figure>

    Description: This graph shows the percentage of reads containing adapter sequences. High levels can interfere with downstream analysis.

    Assessment: The data passes quality control as there is no significant adapter contamination detected.

### b. Quality control of the alignment

For this section we will be using samtools to check the alignment of our
data. \#?# 6. Use samtools flagstat to check the alignment rate of the
sample you ran. Paste the output below (0.5 pts) and explain what the
output means (1.5 pts)

    1000000 + 0 in total (QC-passed reads + QC-failed reads)
    0 + 0 secondary
    0 + 0 supplementary
    0 + 0 duplicates
    924585 + 0 mapped (92.46% : N/A)
    1000000 + 0 paired in sequencing
    500000 + 0 read1
    500000 + 0 read2
    898442 + 0 properly paired (89.84% : N/A)
    914236 + 0 with itself and mate mapped
    10349 + 0 singletons (1.03% : N/A)
    3180 + 0 with mate mapped to a different chr
    1857 + 0 with mate mapped to a different chr (mapQ>=5)

### Part 3: Visualization and Downstream Analysis

#### a. Setting up a reproducible analysis in R

**Reminder that you will be doing your analysis in RStudio on your local
computer.** Before you start analyzing the sequencing data, we will go
over some essential methods for reproducible analysis in R. Generally
installing packages in RStudio is very easy, with packages being
maintained on the CRAN repository. For example to install the common
ggplot visualization package in RStudio:

``` r
install.packages("ggplot2")
```

or to install from Bioconductor, you can use the BiocManager package:

``` r
if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
}
BiocManager::install("<package-name>")
```

However RStudio does not generally save which versions of packages you
had installed at the time of a particular analysis, so if you were
trying to reproduce your analysis at a later date you might run into
serious headaches either updating the code or trying to install
(potentially hundreds of) outdated packages and dependencies. In order
to solve this issue you can use package managers which work similarly to
how conda managed our command line software. In this course we will be
using the renv package.

``` r
install.packages("renv")
```

An R library is a directory on your computer where R stores all the
packages you have installed. You can see where your system library is by
typing `.libPaths()` into your R interpreter. Normally when you type
`install.packages()` into your R interpreter you install (from a
repository like CRAN or Bioconductor) into what’s caled the
system/global library. This is the equivalent of installing software
directly onto your computer instead of within a conda environment in
that the global library will be shared across all projects. This is a
remarkably bad practice with regards to reproduciblity and project-based
analysis.

Once you have renv installed you can create a new library by typing
`renv::init()` into your R interpreter. This will create a new library
in your current working directory. You can then install packages into
this library by typing `renv::install()` into your R interpreter. You
can also install packages from a specific repository by typing
`renv::install("package_name", repos = "https://cran.rstudio.com")` into
your R interpreter. Because renv doesn’t interfere with existing package
management workflows you can still use `install.packages()` and
`BiocManager::install()` to install packages into your project library
so long as you’ve initialized renv in your project.

#### b. Basics of R Analysis

For this section you will want to install tidyverse
(<https://www.tidyverse.org/>), a collection of packages for data
manipulation and visualization. We will need it for the ggplot2 package
which it includes.

\#?# 7. Below is an R function for reading a VCF file and counting the
number of occurrences of each unique ref-alt pair in all of the SNPs
present in the VCF file. There are a few bugs in the code. Debug the
function (2 pts) and add comments to explain what each line of code is
doing (1 pts).

``` r
# NOTE: Delete all the #'s in this block to uncomment the code 
count_SNPs <- function(file_path) {
    # Read the VCF file into a data frame.
    # The VCF file is tab-separated and does not include headers by default in read.table.
    vcf_data <- read.table(file_path, header = FALSE, stringsAsFactors = FALSE, comment.char = "#")

    # Define the column names for the VCF file (adjust these if your VCF file has a different structure).
    # For simplicity, assuming the standard VCF format.
    colnames(vcf_data) <- c("CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO")

    # Extract the "REF" (reference allele) and "ALT" (alternate allele) columns.
    # These columns contain the information about SNPs.
    ref_alt_data <- vcf_data[, c("REF", "ALT")]

    # Filter rows where both REF and ALT alleles are single characters (i.e., SNPs).
    # This excludes multi-nucleotide variations (e.g., indels).
    ref_alt_data <- ref_alt_data[apply(ref_alt_data, 1, function(x) all(nchar(x) == 1)), ]

    # Count occurrences of each unique REF-ALT pair.
    counts <- table(ref_alt_data$REF, ref_alt_data$ALT)

    # Convert the table of counts into a data frame for easier manipulation.
    result_df <- data.frame(
        REF = rep(rownames(counts), each = ncol(counts)),  # Repeat REF values for each ALT.
        ALT = rep(colnames(counts), times = nrow(counts)), # Repeat ALT values for each REF.
        COUNT = as.vector(counts)                         # Flatten the counts table into a vector.
    )

    # Return the resulting data frame containing REF, ALT, and their respective counts.
    return(result_df)
}
```

``` r
result <- count_SNPs("~/Downloads/subset_SRR099957.vcf")
```

\#?# 8. Use the returned data frame to plot the ref-alt pairs for all
SNPs as a bar plot (1 pts)

``` r
# Include the code you used to generate the plot in this block. When you knit your document the plot will be generated and displayed below.
# Load the ggplot2 package for plotting
library(ggplot2)

# Assuming result_df is the data frame returned by the count_SNPs function
plot_ref_alt <- function(result_df) {
    # Create a new column combining REF and ALT for labeling
    result_df$REF_ALT <- paste(result_df$REF, result_df$ALT, sep = "-")
    
    # Create the bar plot
    ggplot(result_df, aes(x = REF_ALT, y = COUNT)) +
        geom_bar(stat = "identity", fill = "steelblue") +  # Bar plot with specified fill color
        theme_minimal() +  # Use a clean theme
        labs(
            title = "Counts of REF-ALT Pairs in SNPs",
            x = "REF-ALT Pair",
            y = "Count"
        ) +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better visibility
}

# Example usage
# Assuming result is the output of count_SNPs function
plot_ref_alt(result)
```

![](A1_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

\#?# 9. Included in the assignment repo is a plot which shows the
distribution of **variants** across the genome. You can download and
view this plot manually. Recreate this plot using the data from your
sample. Include the original svg and your recreation in your submission.
(4 pts)

``` r
# HINT: You'll want to start by looking at the documentation for the GenomicRanges and GenomicDistributions packages.
# We know this question requires a deep dive into the documentation, this is by design as learning how to implement novel tools/packages is a key skill in bioinformatics.
library(GenomicRanges)
```

    ## Loading required package: stats4

    ## Loading required package: BiocGenerics

    ## 
    ## Attaching package: 'BiocGenerics'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     IQR, mad, sd, var, xtabs

    ## The following objects are masked from 'package:base':
    ## 
    ##     anyDuplicated, aperm, append, as.data.frame, basename, cbind,
    ##     colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,
    ##     get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,
    ##     match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,
    ##     Position, rank, rbind, Reduce, rownames, sapply, saveRDS, setdiff,
    ##     table, tapply, union, unique, unsplit, which.max, which.min

    ## Loading required package: S4Vectors

    ## 
    ## Attaching package: 'S4Vectors'

    ## The following object is masked from 'package:utils':
    ## 
    ##     findMatches

    ## The following objects are masked from 'package:base':
    ## 
    ##     expand.grid, I, unname

    ## Loading required package: IRanges

    ## Loading required package: GenomeInfoDb

``` r
library(GenomicDistributions)

# Step 1: Create a GRanges object from your VCF file
vcf_data <- read.table("~/Downloads/subset_SRR099957.vcf", header = FALSE, stringsAsFactors = FALSE, comment.char = "#")

# Assign column names (adjust as necessary for your VCF file)
colnames(vcf_data) <- c("CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO")

# Convert VCF data to a GRanges object
gr <- GRanges(
    seqnames = vcf_data$CHROM,
    ranges = IRanges(start = vcf_data$POS, end = vcf_data$POS),
    REF = vcf_data$REF,
    ALT = vcf_data$ALT
)

# Filter for standard chromosomes
standard_chromosomes <- c(paste0("chr", 1:22), "chrX", "chrY", "chrM")
gr <- gr[seqnames(gr) %in% standard_chromosomes]

x = calcChromBinsRef(gr, "hg38")
```

    ## see ?GenomicDistributionsData and browseVignettes('GenomicDistributionsData') for documentation

    ## loading from cache

``` r
plotChromBins(x)
```

![](A1_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

<figure>
<img src="Chr1_VariantsvsPos.svg" alt="Chr1_VariantsvsPos" />
<figcaption aria-hidden="true">Chr1_VariantsvsPos</figcaption>
</figure>

#### c. Interactively viewing our variants in IGV

You can download the IGV viewer from here:
<https://software.broadinstitute.org/software/igv/home>. IGV is an
interactive tool to visualize different data types of genetic
information (e.g. bam, bed files). You will install this tool to your
**local computer**. To visualize where the reads of our ChIP analysis
mapped in the genome.

Once you boot up IGV select your reference genome and load your VCF
file. Zoom into this position: `chr1: 1,000,000-1,100,000`.

    #?# 10. Post a screenshot of your IGV window below (1pts)

    ![igvScreenshot](/Users/mekail/Downloads/igvScreenshot.png)

    ## Discussion (6 pts)
    #?# 11. Do you think the subset of sequence data you analyzed was from Whole Genome Sequencing or Whole Exome Sequencing? Justify your response with evidence. (4 pts)

    The subset of sequence data analyzed appears to be from Whole Exome Sequencing (WES). Analysis of the BAM file in IGV reveals that the sequencing reads are concentrated in the exonic regions of genes, such as AGRN, as observed in Question 10. This target alignment pattern is common for WES, since it focuses sequencing on exonic regions. Additionally, there is minimal coverage in intergenic or intronic regions, which further supports the conclusion that this is WES. If the data were from Whole Genome Sequencing (WGS), we would expect to see significant read coverage in non-coding regions, as WGS sequences the entire genome. A total of 500,000 sequences aligns far more with WES versus WGS, as WGS typically has sequences ranging in the hundereds of millions to billions of sequences.



    Assuming your snakemake run was supposed to process 1000 samples instead of 1 sample. 


    ``` bash
    #?# 12. How would you go about checking the quality of all the samples? (1 pts)
    To check the quality of 1000 samples, using an automated pipeline with snakemake could be a suitable approach. Then it would follow similar steps such as, running fastqc on all samples, then results could be aggregated using multiqc for a single summary report for easier inspection. Trimmomatic could be used to remove low quality bases and then FastQC could be rerun. Tools like BWA could be used to check alignment, and provide metrics like the mapping rate or coverage depth. Various other tools could be used to check other things like duplication rates, a contamination check, as well as variant calling.

``` bash
#?# 13. If the run crashed on sample 500, how would you go about restarting the run? (1 pts)
Due to snakemake’s checkpoint functions, the run could be restarted from where it crashed on sample 500. However a deeper look into why it crashed would help prevent the issue form reoccurring. In that case, the log files could be analyzed to help identify what caused the failure in sample 500. Once the issue is identified, the run could be resumed using snakemakes rerun option, ensuring that only the incomplete or failed jobs (sample 500) are rerun while the already completed ones are skipped. This approach makes reprocessing all 1000 samples more efficient.
```

# Contributions

Please note here the team members and their contributions to this
assignment.

Mekail (35922863): All work collaborated on for the entirety of the
assignment. Caden(56684616): All work collaborated on for the entirety
of the assignment.

# References:

<https://docs.varsome.com/en/fastqc-report>
<https://www.bioconductor.org/packages/release/bioc/html/GenomicDistributions.html>
<https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html>
<https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/samtools/sort.html>
<https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/samtools/index.html>

ChatGPT Chat:
<https://chatgpt.com/share/678f3daf-4858-800d-bf8d-b52526cd5bb8>
